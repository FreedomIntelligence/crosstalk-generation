# 人工评分详细说明



#### 1.人工评分数据文件说明：

```
data:
|-------generate_completions.json     各个模型根据meta_prompt中的提示生成的后10句相声文本
|-------meta_prompt.json							人工筛选的50篇对口相声的前10句
|-------score_records.json            人工评分的记录
|-------user_list.json                所有注册了评分任务的人员信息
```



#### 2.人工评分数据获得的全流程描述：

2.1. 先人工筛选50篇对口相声，截取了前20句，并将前10句作为prompt存入meta_prompt表。

2.2.使用所有的模型（9个）输入prompt生成后续文本，再将之前人工节选的50篇中的后10句混入其中（人工评分数据的生成均使用输入前10句，生成后10句的方式），共获得10组待评分数据。generate_completions.json中的type对应其模型类别，模型类别对应如下：

```
    1: "真实数据",
    2:"rnn",
    3:"GPT",
    4:"unilm",
    5:"zhouwenwang",
    6:"T5",
    7:"GPT3",
    8:"GPT3-finetune",
    9:"CPM",
    10:"PANGU-a"
```

2.3.开发一个带web页面的评分系统，邀请不同年龄，职业，性别的被试来进行相声片段的评分，评分维度有：

```
综合得分（0~5） | 幽默度得分（0~5） | 通顺度得分（0,1） | 歧视程度（0,1）|

综合得分和幽默得分为5分制，通顺度与歧视为1分制（是则1，否则0）
```

2.4.每个评分用户会分配5个评分篇章，每个评分篇章为根据同一个prompt，使用不同模型所生成的10个后续文本（其中混杂了真实样本）。

2.5.我们共邀请了60位被试进行了评分，其中在截止期限完成了全部评分项目的共有30位，所有的评分记录都在score_records.json文件中，我们只筛选了完成全部评分项目的评分记录，经统计后得到了如下结果：



| 括号中为真实数据得分-> | 综合得分(528) | 幽默度得分（519） | 通顺度得分(143) | 歧视程度(3) |
| ---------------------- | ------------- | ----------------- | --------------- | ----------- |
| GPT-ep50               | 225           | 256               | 59              | 2           |
| T5-pesg-ep15           | 270           | 296               | 76              | 7           |
| CPM_large              | 213           | 240               | 60              | **34**      |
| UNILM_ep45             | 276           | 301               | 84              | 2           |
| RNN                    | 217           | 242               | 41              | 4           |
| GPT3-base-Davinci      | 322           | 325               | 98              | 5           |
| GPT3-ft200-Davinci     | **341**       | **353**           | **106**         | 2           |
| Panggu-a               | 230           | 257               | 63              | 4           |
| zhouwenwang            | 184           | 191               | 28              | 8           |

该结果中的数据可通过同目录下的 human_metrics.py 进行复现。
