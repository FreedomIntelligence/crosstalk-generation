# Manual scoring details



#### 1.Manual Scoring Data File Description：

```
data:
|-------generate_completions.json     The last 10 sentences of cross talk text generated by each model according to the prompts in meta_prompt
|-------meta_prompt.json				The first 10 sentences of 50 cross-talk dialogues selected manually
|-------score_records.json            Manually graded records
|-------user_list.json                Information of all people who have registered for grading tasks
```



#### 2.Full process description of manual scoring data acquisition：

2.1. First manually screened 50 crosstalk articles, intercepted the first 20 sentences, and stored the first 10 sentences as prompts in the meta_prompt table。

2.2.Use all models (9) to input prompts to generate subsequent texts, and then mix the last 10 sentences of the 50 manual excerpts into it (the generation of manual scoring data uses the method of inputting the first 10 sentences and generating the last 10 sentences), A total of 10 sets of data to be scored were obtained. The type in generate_completions.json corresponds to its model category, and the model category corresponds to the following：

```
    1: "real data",
    2:"rnn",
    3:"GPT",
    4:"unilm",
    5:"zhouwenwang",
    6:"T5",
    7:"GPT3",
    8:"GPT3-finetune",
    9:"CPM",
    10:"PANGU-a"
```

2.3.Develop a scoring system with a web page, and invite subjects of different ages, occupations, and genders to score cross talk clips. The scoring dimensions are：

```
overall ratings（0~5） | humor score（0~5） | Fluency Score（0,1） | Degree of discrimination（0,1）|

The overall score and humor score are on a 5-point scale, and the fluency and discrimination are on a 1-point scale (1 if yes, 0 otherwise)
```

2.4.Each scoring user will be assigned 5 scoring chapters, and each scoring chapter is 10 subsequent texts (mixed with real samples) generated by different models based on the same prompt.

2.5.We invited a total of 60 subjects to score, of which 30 completed all scoring items by the deadline. All scoring records are in the score_records.json file. We only screened the scoring records that completed all scoring items. After statistics, the following results were obtained：



| The real data scores are in parentheses-> | overall ratings(528) | humor score（519） | Fluency Score(143) | Degree of discrimination(3) |
| ---------------------- | ------------- | ----------------- | --------------- | ----------- |
| GPT-ep50               | 225           | 256               | 59              | 2           |
| T5-pesg-ep15           | 270           | 296               | 76              | 7           |
| CPM_large              | 213           | 240               | 60              | **34**      |
| UNILM_ep45             | 276           | 301               | 84              | 2           |
| RNN                    | 217           | 242               | 41              | 4           |
| GPT3-base-Davinci      | 322           | 325               | 98              | 5           |
| GPT3-ft200-Davinci     | **341**       | **353**           | **106**         | 2           |
| Panggu-a               | 230           | 257               | 63              | 4           |
| zhouwenwang            | 184           | 191               | 28              | 8           |

The data in this result can be reproduced by human_metrics.py in the same directory
